{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weNlYvvN9I_y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('wdbc.data', header=None)\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhJ3mPcj99bP",
        "outputId": "3403059e-12f8-43ef-aee2-dff37d3dc8b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         0  1      2      3       4       5        6        7       8   \\\n",
            "0    842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001   \n",
            "1    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
            "2  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
            "3  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
            "4  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
            "\n",
            "        9   ...     22     23      24      25      26      27      28      29  \\\n",
            "0  0.14710  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
            "1  0.07017  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
            "2  0.12790  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
            "3  0.10520  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
            "4  0.10430  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
            "\n",
            "       30       31  \n",
            "0  0.4601  0.11890  \n",
            "1  0.2750  0.08902  \n",
            "2  0.3613  0.08758  \n",
            "3  0.6638  0.17300  \n",
            "4  0.2364  0.07678  \n",
            "\n",
            "[5 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load data\n",
        "columns = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n",
        "           'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave_points_mean',\n",
        "           'symmetry_mean', 'fractal_dimension_mean'] + [f'feat_{i}' for i in range(20)]\n",
        "\n",
        "df = pd.read_csv('wdbc.data', names=columns)\n",
        "\n",
        "# Preprocess\n",
        "X = df.drop(['id', 'diagnosis'], axis=1)\n",
        "y = LabelEncoder().fit_transform(df['diagnosis']) # M -> 1, B -> 0\n",
        "\n",
        "# Split and Scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "vB3TiiBs-ExG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define Descriptive Column Names\n",
        "# The dataset contains 10 base features measured as Mean, SE, and Worst (30 total)\n",
        "base_features = [\n",
        "    'radius', 'texture', 'perimeter', 'area', 'smoothness',\n",
        "    'compactness', 'concavity', 'concave_points', 'symmetry', 'fractal_dimension'\n",
        "]\n",
        "\n",
        "# Construct the full list of 30 feature names\n",
        "feature_names = [f\"{name}_mean\" for name in base_features] + \\\n",
        "                [f\"{name}_se\" for name in base_features] + \\\n",
        "                [f\"{name}_worst\" for name in base_features]\n",
        "\n",
        "# Full column list for the .data file (ID + Diagnosis + 30 Features)\n",
        "columns = ['id', 'diagnosis'] + feature_names\n",
        "\n",
        "# 2. Load Data\n",
        "df = pd.read_csv('wdbc.data', names=columns)\n",
        "\n",
        "# 3. Preprocess\n",
        "# We keep X as a DataFrame to preserve the feature names during training\n",
        "X = df.drop(['id', 'diagnosis'], axis=1)\n",
        "y = LabelEncoder().fit_transform(df['diagnosis']) # M -> 1, B -> 0\n",
        "\n",
        "# 4. Split and Scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Important: StandardScaler returns a numpy array, so we convert it back\n",
        "# to a DataFrame to keep the feature names attached\n",
        "scaler = StandardScaler()\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=feature_names)\n",
        "X_test = pd.DataFrame(scaler.transform(X_test), columns=feature_names)\n",
        "\n"
      ],
      "metadata": {
        "id": "c_sCvn3LPmVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(name, model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1] # Required for AUC\n",
        "\n",
        "    return {\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(y_test, y_proba),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1\": f1_score(y_test, y_pred),\n",
        "        \"MCC\": matthews_corrcoef(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "# Initialize Models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"K-Nearest Neighbor\": KNeighborsClassifier(n_neighbors=5),\n",
        "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Train and Evaluate\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    results.append(evaluate_model(name, model, X_test, y_test))\n",
        "\n",
        "# Display Results\n",
        "perf_df = pd.DataFrame(results)\n",
        "print(perf_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpvv_cs5_C4L",
        "outputId": "022f6cbf-cb25-4d79-dc9e-253c634b7c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [15:52:18] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  Model  Accuracy       AUC  Precision    Recall        F1  \\\n",
            "0   Logistic Regression  0.973684  0.997380   0.976190  0.953488  0.964706   \n",
            "1         Decision Tree  0.947368  0.943990   0.930233  0.930233  0.930233   \n",
            "2    K-Nearest Neighbor  0.947368  0.981985   0.930233  0.930233  0.930233   \n",
            "3  Gaussian Naive Bayes  0.964912  0.997380   0.975610  0.930233  0.952381   \n",
            "4         Random Forest  0.964912  0.995742   0.975610  0.930233  0.952381   \n",
            "5               XGBoost  0.956140  0.990829   0.952381  0.930233  0.941176   \n",
            "\n",
            "        MCC  \n",
            "0  0.943898  \n",
            "1  0.887979  \n",
            "2  0.887979  \n",
            "3  0.925285  \n",
            "4  0.925285  \n",
            "5  0.906379  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, save the models using the model objects from the dictionary\n",
        "joblib.dump(models[\"Logistic Regression\"], 'breast_cancer_model_lr.pkl')\n",
        "joblib.dump(models[\"Decision Tree\"], 'breast_cancer_model_dt.pkl')\n",
        "joblib.dump(models[\"XGBoost\"], 'breast_cancer_model_xg.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZOmkW0R5_z2",
        "outputId": "5d89c330-7d35-4768-9c70-b467a844e964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scaler.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQDUcN938iyt",
        "outputId": "fed47f04-3bee-46a0-98eb-02f2c7545f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
              "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
              "       'concave_points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
              "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
              "       'compactness_se', 'concavity_se', 'concave_points_se', 'symmetry_se',\n",
              "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
              "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
              "       'compactness_worst', 'concavity_worst', 'concave_points_worst',\n",
              "       'symmetry_worst', 'fractal_dimension_worst'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}